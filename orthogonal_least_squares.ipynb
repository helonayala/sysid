{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5Wr+HakyHbIxwbwhsAGrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helonayala/sysid/blob/main/orthogonal_least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orthogonal least squares\n",
        "\n",
        "This script shows the use of OLS for generating ERR metrics.\n",
        "\n",
        "For didatic purposes, we show the applications of OLS to a small matrix, as given in an example from Billings (2013) book: data from Table 3.1 (Example 3.3).\n",
        "\n",
        "The matrix is given below:\n",
        "\n",
        "| x_1 | x_2 | x_3 | x_4   | y.   |\n",
        "\n",
        "| 9   | -5  | 5   | -1.53 | 9.08 |\n",
        "\n",
        "| 1   | -1  | 8   | -0.39 | 7.87 |\n",
        "\n",
        "| 2   | -5  | 6   | -3.26 | 3.01 |\n",
        "\n",
        "| 8   | -2  | 0   | 0.36  | 5.98 |\n",
        "\n",
        "| 0   | 0   | 9   | 0.13  | 9.05 |\n",
        "\n",
        "The last column is the output, and x_4 is a linear combination of x_1 and x_2. The ideal model would use only 3 columns instead of 4. We show below how OLS can be used to detect redundant information."
      ],
      "metadata": {
        "id": "dmfEQcp68IJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions and imports"
      ],
      "metadata": {
        "id": "y0t1wkD1MNr5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GdbqizoA32RK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import pinv, solve\n",
        "\n",
        "def MGS(P):\n",
        "    \"\"\"\n",
        "    Performs Modified Gram-Schmidt orthogonalization on matrix P,\n",
        "    as defined in Aguirre (2015).\n",
        "\n",
        "    Args:\n",
        "        P (np.ndarray): The input matrix where columns are vectors to be orthogonalized.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            'Q' (np.ndarray): The orthogonalized matrix, where columns are orthogonal.\n",
        "            'A' (np.ndarray): The unit upper triangular matrix of coefficients.\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = P.shape\n",
        "\n",
        "    A = np.eye(n_cols, dtype=float) # Initialize A as an identity matrix (unit upper triangular)\n",
        "    P_curr = P.astype(float)       # Working copy of P, converted to float\n",
        "    Q = np.zeros_like(P_curr, dtype=float)\n",
        "\n",
        "    # Iterate through columns to be orthogonalized\n",
        "    for i in range(n_cols):\n",
        "        Q[:, i] = P_curr[:, i] # The i-th orthogonal vector Q[:,i] is the current P_curr[:,i]\n",
        "\n",
        "        # Orthogonalize subsequent columns (P_curr[:,j]) against the current orthogonal vector Q[:,i]\n",
        "        # This loop applies the modification step for each subsequent column\n",
        "        for j in range(i + 1, n_cols):\n",
        "            # Check for zero norm to prevent division by zero for orthogonal vector Q[:,i]\n",
        "            # If Q[:,i] is a zero vector, its projection onto other vectors is zero,\n",
        "            # so A[i,j] remains 0 and P_curr[:,j] doesn't change from this step.\n",
        "            q_i_norm_sq = Q[:, i].T @ Q[:, i]\n",
        "            if q_i_norm_sq > 1e-18: # Use a small epsilon to check for non-zero norm\n",
        "                # Compute coefficient A[i,j] (projection of P_curr[:,j] onto Q[:,i])\n",
        "                A[i, j] = (Q[:, i].T @ P_curr[:, j]) / q_i_norm_sq\n",
        "                # Subtract the projection from P_curr[:,j]\n",
        "                P_curr[:, j] = P_curr[:, j] - A[i, j] * Q[:, i]\n",
        "            # If q_i_norm_sq is zero, A[i,j] is already 0 (from identity init) and P_curr[:,j] remains unchanged\n",
        "\n",
        "    return Q, A\n",
        "\n",
        "def ols(P, Y_target):\n",
        "    \"\"\"\n",
        "    Calculates OLS parameters, Error Reduction Ratio (ERR), and Error Sum Ratio (ESR)\n",
        "    based on the specified column indices from the full predictor matrix P.\n",
        "\n",
        "    Args:\n",
        "        P_full (np.ndarray): The full original predictor matrix (e.g., P_original).\n",
        "        Y_target (np.ndarray): The target vector (e.g., Y).\n",
        "        p_column_indices (list or np.ndarray): A list or array of 0-based integer\n",
        "                                                indices specifying which columns\n",
        "                                                from P_full to use for the current P matrix.\n",
        "        mgs_function (function): The Modified Gram-Schmidt function to use (e.g., MGS).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            th_OLS (np.ndarray): OLS estimated parameters.\n",
        "            ERR (np.ndarray): Error Reduction Ratio for each orthogonal regressor.\n",
        "            ESR (float): Error Sum Ratio (total unexplained variance).\n",
        "    \"\"\"\n",
        "\n",
        "    niter = P.shape[1] # Number of regressors (columns in the sliced P matrix)\n",
        "\n",
        "    # Perform Modified Gram-Schmidt orthogonalization on the sliced P matrix.\n",
        "    W, A = MGS(P)\n",
        "\n",
        "    # Calculate 'g' coefficients in the orthogonal basis.\n",
        "    g = np.zeros(niter)\n",
        "    for i in range(niter):\n",
        "        g[i] = (Y_target.T @ W[:, i]).item() / (W[:, i].T @ W[:, i]).item()\n",
        "    g = g.reshape(-1, 1) # Reshape 'g' to a column vector\n",
        "\n",
        "    # Calculate ERR (Error Reduction Ratio) for each regressor.\n",
        "    ERR = np.zeros(niter)\n",
        "    for i in range(niter):\n",
        "        ERR[i] = ((Y_target.T @ W[:, i]).item()**2) / ((Y_target.T @ Y_target).item() * (W[:, i].T @ W[:, i]).item())\n",
        "\n",
        "    # Calculate ESR (Error Sum Ratio).\n",
        "    ESR = 1 - np.sum(ERR)\n",
        "\n",
        "    # Calculate the final OLS parameters (th_OLS) from the orthogonal basis.\n",
        "    th_OLS = solve(A, g)\n",
        "\n",
        "    return th_OLS, ERR, ESR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "VOkzmSyFMSpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Data from Table 3.1 (Billings 2013 book, Example 3.3) ---\n",
        "mat_data = np.array([\n",
        "    [9, -5, 5, -1.53, 9.08],\n",
        "    [1, -1, 8, -0.39, 7.87],\n",
        "    [2, -5, 6, -3.26, 3.01],\n",
        "    [8, -2, 0, 0.36, 5.98],\n",
        "    [0, 0, 9, 0.13, 9.05]\n",
        "])\n",
        "\n",
        "# Separate predictors (P) and output (Y)\n",
        "P_original = mat_data[:, :4]\n",
        "Y = mat_data[:, 4].reshape(-1, 1)"
      ],
      "metadata": {
        "id": "odExlaLVMRow"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Least Squares Solution\n",
        "\n",
        "Calculate the LS solution using the pseudo-inverse (generalized inverse). This is given just after Eq. (3.22) in the book."
      ],
      "metadata": {
        "id": "ICUJ1qhKN-Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The formula is: theta_hat = (P^T P)^-1 P^T Y, which is equivalent to pinv(P) @ Y.\n",
        "th_ls = pinv(P_original) @ Y\n",
        "print('OLS estimated parameters (th_ls):')\n",
        "print(th_ls)"
      ],
      "metadata": {
        "id": "JrA9XsD5N85j",
        "outputId": "997c42a9-9cfa-4b82-b5ab-23edd4996744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS estimated parameters (th_ls):\n",
            "[[0.85685332]\n",
            " [0.5363392 ]\n",
            " [0.98734167]\n",
            " [0.59768453]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orthogonal Least Squares (OLS) Solution\n",
        "\n",
        "As in Table 3.2 we try to compare the solution given by this code to the content in the book.\n",
        "\n",
        "Two-terms model (1st line in Table 3.2)\n"
      ],
      "metadata": {
        "id": "f6m_jhTNOKfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "th_OLS, ERR, ESR = ols(P_original[:,[2, 0]], Y)\n",
        "\n",
        "print('OLS estimated parameters (th_OLS from OLS method using MGS):')\n",
        "print(th_OLS)\n",
        "print('\\nERR (Error Reduction Ratio) for each orthogonal regressor:')\n",
        "print(ERR)\n",
        "print('\\nESR (Error Sum Ratio):')\n",
        "print(ESR)\n",
        "print(\"-\" * 40 + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "L03-ePqPOJh6",
        "outputId": "2139a395-d859-4aa8-8738-4c699042638d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS estimated parameters (th_OLS from OLS method using MGS):\n",
            "[[0.81935333]\n",
            " [0.60128022]]\n",
            "\n",
            "ERR (Error Reduction Ratio) for each orthogonal regressor:\n",
            "[0.77370749 0.17268374]\n",
            "\n",
            "ESR (Error Sum Ratio):\n",
            "0.053608771292348534\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three-terms model (2nd line in Table 3.2)"
      ],
      "metadata": {
        "id": "YRvo-JptSqj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "th_OLS, ERR, ESR = ols(P_original[:,[2, 0, 1]], Y)\n",
        "\n",
        "print('OLS estimated parameters (th_OLS from OLS method using MGS):')\n",
        "print(th_OLS)\n",
        "print('\\nERR (Error Reduction Ratio) for each orthogonal regressor:')\n",
        "print(ERR)\n",
        "print('\\nESR (Error Sum Ratio):')\n",
        "print(ESR)\n",
        "print(\"-\" * 40 + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LsQQeCZuSsnw",
        "outputId": "a7d87ecd-9ec2-4a29-bbbc-76e433aaf3f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS estimated parameters (th_OLS from OLS method using MGS):\n",
            "[[0.99669235]\n",
            " [1.00046032]\n",
            " [0.99172293]]\n",
            "\n",
            "ERR (Error Reduction Ratio) for each orthogonal regressor:\n",
            "[0.77370749 0.17268374 0.05352266]\n",
            "\n",
            "ESR (Error Sum Ratio):\n",
            "8.611116850232303e-05\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Four-terms model (3rd line in Table 3.2, includes a redundant term, with low ERR)"
      ],
      "metadata": {
        "id": "GpTk9fgsSs6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_OLS, ERR, ESR = ols(P_original[:,[2, 0, 1, 3]], Y)\n",
        "\n",
        "print('OLS estimated parameters (th_OLS from OLS method using MGS):')\n",
        "print(th_OLS)\n",
        "print('\\nERR (Error Reduction Ratio) for each orthogonal regressor:')\n",
        "print(ERR)\n",
        "print('\\nESR (Error Sum Ratio):')\n",
        "print(ESR)\n",
        "print(\"-\" * 40 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "ea7GqoT0Sw7C",
        "outputId": "6a8fd60e-f8c2-4916-f118-f1796ef48534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS estimated parameters (th_OLS from OLS method using MGS):\n",
            "[[0.98734167]\n",
            " [0.85685332]\n",
            " [0.5363392 ]\n",
            " [0.59768453]]\n",
            "\n",
            "ERR (Error Reduction Ratio) for each orthogonal regressor:\n",
            "[7.73707491e-01 1.72683737e-01 5.35226601e-02 4.95380005e-06]\n",
            "\n",
            "ESR (Error Sum Ratio, total unexplained variance):\n",
            "8.115736845359933e-05\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results should be directly comparable to Table 3.2 in Billing's book.\n",
        "\n",
        "Adding the last term did not change significantly the ESR."
      ],
      "metadata": {
        "id": "m1uFC6c0Pw0V"
      }
    }
  ]
}