{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8kh97timxNvDd1BJrb7o0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helonayala/sysid/blob/main/orthogonal_least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdbqizoA32RK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import pinv, solve\n",
        "\n",
        "# --- Modified Gram-Schmidt (MGS) Orthogonalization Function ---\n",
        "# This implementation is based on Aguirre 2015.\n",
        "# It produces an orthogonal matrix Q (columns are orthogonal,\n",
        "# but not necessarily unit norm) and a unit upper triangular matrix A, such that P = Q @ A.\n",
        "def MGS(P):\n",
        "    \"\"\"\n",
        "    Performs Modified Gram-Schmidt orthogonalization on matrix P,\n",
        "    as defined in Aguirre (2015).\n",
        "\n",
        "    Args:\n",
        "        P (np.ndarray): The input matrix where columns are vectors to be orthogonalized.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            'Q' (np.ndarray): The orthogonalized matrix, where columns are orthogonal.\n",
        "            'A' (np.ndarray): The unit upper triangular matrix of coefficients.\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = P.shape\n",
        "\n",
        "    A = np.eye(n_cols, dtype=float) # Initialize A as an identity matrix (unit upper triangular)\n",
        "    P_curr = P.astype(float)       # Working copy of P, converted to float\n",
        "    Q = np.zeros_like(P_curr, dtype=float)\n",
        "\n",
        "    # Iterate through columns to be orthogonalized\n",
        "    for i in range(n_cols):\n",
        "        Q[:, i] = P_curr[:, i] # The i-th orthogonal vector Q[:,i] is the current P_curr[:,i]\n",
        "\n",
        "        # Orthogonalize subsequent columns (P_curr[:,j]) against the current orthogonal vector Q[:,i]\n",
        "        # This loop applies the modification step for each subsequent column\n",
        "        for j in range(i + 1, n_cols):\n",
        "            # Check for zero norm to prevent division by zero for orthogonal vector Q[:,i]\n",
        "            # If Q[:,i] is a zero vector, its projection onto other vectors is zero,\n",
        "            # so A[i,j] remains 0 and P_curr[:,j] doesn't change from this step.\n",
        "            q_i_norm_sq = Q[:, i].T @ Q[:, i]\n",
        "            if q_i_norm_sq > 1e-18: # Use a small epsilon to check for non-zero norm\n",
        "                # Compute coefficient A[i,j] (projection of P_curr[:,j] onto Q[:,i])\n",
        "                A[i, j] = (Q[:, i].T @ P_curr[:, j]) / q_i_norm_sq\n",
        "                # Subtract the projection from P_curr[:,j]\n",
        "                P_curr[:, j] = P_curr[:, j] - A[i, j] * Q[:, i]\n",
        "            # If q_i_norm_sq is zero, A[i,j] is already 0 (from identity init) and P_curr[:,j] remains unchanged\n",
        "\n",
        "    return {'Q': Q, 'A': A}\n",
        "\n",
        "# --- Data from Table 3.1 (Billings 2013 book, Example 3.3) ---\n",
        "# This matrix corresponds to the 'Mat' variable in the original R code.\n",
        "mat_data = np.array([\n",
        "    [9, -5, 5, -1.53, 9.08],\n",
        "    [1, -1, 8, -0.39, 7.87],\n",
        "    [2, -5, 6, -3.26, 3.01],\n",
        "    [8, -2, 0, 0.36, 5.98],\n",
        "    [0, 0, 9, 0.13, 9.05]\n",
        "])\n",
        "\n",
        "# Separate predictors (P) and output (Y)\n",
        "P_original = mat_data[:, :4]\n",
        "Y = mat_data[:, 4].reshape(-1, 1)\n",
        "\n",
        "print('--- Ordinary Least Squares (OLS) Solution ---')\n",
        "# Calculate the OLS solution using the pseudo-inverse (generalized inverse).\n",
        "# This is equivalent to 'th_ls = ginv(P) %*% Y' in the R code.\n",
        "# The formula is: theta_hat = (P^T P)^-1 P^T Y, which is equivalent to pinv(P) @ Y.\n",
        "th_ls = pinv(P_original) @ Y\n",
        "print('OLS estimated parameters (th_ls):')\n",
        "print(th_ls)\n",
        "\n",
        "print('\\n--- Orthogonal Least Squares (OLS) Solution ---')\n",
        "print('Executing for example 3.3 in billings 2013 book')\n",
        "print('Select which line (1, 2 or 3) in table 3.2 you want to check:')\n",
        "print('1: P[:, [2, 0]] (corresponds to R\\'s P[,c(3,1)] - using columns 3 and 1 from original P)')\n",
        "print('2: P[:, [2, 0, 1]] (corresponds to R\\'s P[,c(3,1,2)] - using columns 3, 1, and 2 from original P)')\n",
        "print('3: P[:, [2, 0, 1, 3]] (corresponds to R\\'s P[,c(3,1,2,4)] - using columns 3, 1, 2, and 4 from original P)')\n",
        "\n",
        "# In a standard .py file, you'd typically use input() for user interaction.\n",
        "# For direct execution and reproducibility, a default choice is set.\n",
        "# Uncomment the 'choice = input(...)' line and comment out the 'choice = ...' line below it\n",
        "# if you want interactive input when running from a terminal.\n",
        "# choice = input('Enter your choice (1, 2, or 3): ')\n",
        "choice = '1' # Default choice for direct execution. Change this to '2' or '3' to test other cases.\n",
        "\n",
        "# Select specific columns of P based on the user's choice,\n",
        "# mirroring the 'switch' statement in the R code.\n",
        "if choice == '1':\n",
        "    P = P_original[:, [2, 0]] # Python indices 2 and 0 correspond to R's 3 and 1\n",
        "elif choice == '2':\n",
        "    P = P_original[:, [2, 0, 1]] # Python indices 2, 0, 1 correspond to R's 3, 1, 2\n",
        "elif choice == '3':\n",
        "    P = P_original[:, [2, 0, 1, 3]] # Python indices 2, 0, 1, 3 correspond to R's 3, 1, 2, 4\n",
        "else:\n",
        "    print(\"Invalid choice. Using default choice '1'.\")\n",
        "    P = P_original[:, [2, 0]]\n",
        "\n",
        "niter = P.shape[1] # Number of regressors (columns in the selected P matrix)\n",
        "\n",
        "# Perform Modified Gram-Schmidt orthogonalization on the selected P matrix.\n",
        "out = MGS(P)\n",
        "W = out['Q'] # Orthogonalized matrix (columns are orthogonal, not necessarily unit norm)\n",
        "A = out['A'] # Unit upper triangular matrix from the MGS decomposition\n",
        "\n",
        "# Calculate Alpha (W.T @ W).\n",
        "# Since W has orthogonal (not orthonormal) columns, Alpha will be a diagonal matrix\n",
        "# where the diagonal elements are the squared norms of the columns of W.\n",
        "Alpha = W.T @ W\n",
        "print('\\nAlpha (W.T @ W, should be diagonal for orthogonal W):')\n",
        "print(Alpha)\n",
        "\n",
        "# Calculate 'g' coefficients in the orthogonal basis.\n",
        "# This calculates the projection of Y onto each orthogonal vector in W.\n",
        "g = np.zeros(niter)\n",
        "for i in range(niter):\n",
        "    # (Y.T @ W[:, i]) is the dot product (projection) of Y onto W[:,i]\n",
        "    # (W[:, i].T @ W[:, i]) is the squared norm of W[:,i].\n",
        "    g[i] = (Y.T @ W[:, i]) / (W[:, i].T @ W[:, i])\n",
        "\n",
        "g = g.reshape(-1, 1) # Reshape 'g' to a column vector for matrix operations\n",
        "print('\\ng (coefficients in orthogonal basis):')\n",
        "print(g)\n",
        "\n",
        "# Calculate 'g2' as an alternative verification for 'g'.\n",
        "# This is equivalent to 'solve(Alpha) %*% t(W) %*% Y' in R.\n",
        "# Since Alpha is diagonal, solve(Alpha, ...) effectively scales W.T @ Y.\n",
        "g2 = solve(Alpha, W.T @ Y)\n",
        "print('\\ng2 (alternative calculation of g, should be same as g):')\n",
        "print(g2)\n",
        "\n",
        "# Calculate ERR (Error Reduction Ratio) for each regressor.\n",
        "# ERR measures the proportion of the total output variance explained by each orthogonal regressor.\n",
        "ERR = np.zeros(niter)\n",
        "for i in range(niter):\n",
        "    # Y.T @ Y is the sum of squares of Y (total variance before normalization).\n",
        "    # ((Y.T @ W[:, i])**2) is the squared projection of Y onto W[:,i].\n",
        "    # (W[:, i].T @ W[:, i]) is the squared norm of W[:,i].\n",
        "    ERR[i] = ((Y.T @ W[:, i])**2) / ((Y.T @ Y) * (W[:, i].T @ W[:, i]))\n",
        "\n",
        "# Calculate ESR (Error Sum Ratio).\n",
        "# ESR is 1 minus the sum of ERR, representing the unexplained variance.\n",
        "ESR = 1 - np.sum(ERR)\n",
        "\n",
        "# Calculate the final OLS parameters (th_OLS) from the orthogonal basis.\n",
        "# This is equivalent to 'th_OLS = solve(A,g)' in R.\n",
        "# It converts the coefficients from the orthogonal basis back to the original regressor basis.\n",
        "th_OLS = solve(A, g)\n",
        "\n",
        "print('\\n--- Final Results ---')\n",
        "print('OLS estimated parameters (th_OLS from OLS method using Gram-Schmidt):')\n",
        "print(th_OLS)\n",
        "print('\\nERR (Error Reduction Ratio) for each orthogonal regressor:')\n",
        "print(ERR)\n",
        "print('\\nESR (Error Sum Ratio, total unexplained variance):')\n",
        "print(ESR)"
      ]
    }
  ]
}